{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from basicsr.archs.ddcolor_arch import DDColor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageColorizationPipeline(object):\n",
    "\n",
    "    def __init__(self, model_path, input_size=256, model_size='large'):\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        if model_size == 'tiny':\n",
    "            self.encoder_name = 'convnext-t'\n",
    "        else:\n",
    "            self.encoder_name = 'convnext-l'\n",
    "\n",
    "        self.decoder_type = \"MultiScaleColorDecoder\"\n",
    "\n",
    "        if self.decoder_type == 'MultiScaleColorDecoder':\n",
    "            self.model = DDColor(\n",
    "                encoder_name=self.encoder_name,\n",
    "                decoder_name='MultiScaleColorDecoder',\n",
    "                input_size=[self.input_size, self.input_size],\n",
    "                num_output_channels=2,\n",
    "                last_norm='Spectral',\n",
    "                do_normalize=False,\n",
    "                num_queries=100,\n",
    "                num_scales=3,\n",
    "                dec_layers=9,\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.model = DDColor(\n",
    "                encoder_name=self.encoder_name,\n",
    "                decoder_name='SingleColorDecoder',\n",
    "                input_size=[self.input_size, self.input_size],\n",
    "                num_output_channels=2,\n",
    "                last_norm='Spectral',\n",
    "                do_normalize=False,\n",
    "                num_queries=256,\n",
    "            ).to(self.device)\n",
    "\n",
    "        self.model.load_state_dict(\n",
    "            torch.load(model_path, map_location=torch.device('cpu'))['params'],\n",
    "            strict=False)\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def process(self, img):\n",
    "        self.height, self.width = img.shape[:2]\n",
    "        # print(self.width, self.height)\n",
    "        # if self.width * self.height < 100000:\n",
    "        #     self.input_size = 256\n",
    "\n",
    "        img = (img / 255.0).astype(np.float32)\n",
    "        orig_l = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)[:, :, :1]  # (h, w, 1)\n",
    "\n",
    "        # resize rgb image -> lab -> get grey -> rgb\n",
    "        img = cv2.resize(img, (self.input_size, self.input_size))\n",
    "        img_l = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)[:, :, :1]\n",
    "        img_gray_lab = np.concatenate((img_l, np.zeros_like(img_l), np.zeros_like(img_l)), axis=-1)\n",
    "        img_gray_rgb = cv2.cvtColor(img_gray_lab, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        tensor_gray_rgb = torch.from_numpy(img_gray_rgb.transpose((2, 0, 1))).float().unsqueeze(0).to(self.device)\n",
    "        output_ab = self.model(tensor_gray_rgb).cpu()  # (1, 2, self.height, self.width)\n",
    "\n",
    "        # resize ab -> concat original l -> rgb\n",
    "        output_ab_resize = F.interpolate(output_ab, size=(self.height, self.width))[0].float().numpy().transpose(1, 2, 0)\n",
    "        output_lab = np.concatenate((orig_l, output_ab_resize), axis=-1)\n",
    "        output_bgr = cv2.cvtColor(output_lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "        output_img = (output_bgr * 255.0).round().astype(np.uint8)    \n",
    "\n",
    "        return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path):\n",
    "    print(f\"Extracting frames from Video...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_frames(frames, colorizer):\n",
    "    print(f\"Processing Frames from video\")\n",
    "    processed_frames = []\n",
    "    for frame in frames:\n",
    "        processed_frame = colorizer.process(frame)\n",
    "        processed_frames.append(processed_frame)\n",
    "    return processed_frames\n",
    "\n",
    "def create_video(processed_frames, output_path, fps=30):\n",
    "    print(f\"Creating the video\")\n",
    "    height, width, _ = processed_frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    for frame in processed_frames:\n",
    "        video.write(frame)\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model_path = '/home/raghuram/DDColor/modelscope/damo/cv_ddcolor_image-colorization/pytorch_model.pt'\n",
    "    input_path = 'DDColor/assets/test_images'\n",
    "    output_path = 'results'\n",
    "    input_size = 512\n",
    "    model_size = 'large'\n",
    "    video_output_path = '/home/raghuram/ddcolor_dl_final/colorize_output'\n",
    "\n",
    "    print(f'Output path: {output_path}')\n",
    "\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    colorizer = ImageColorizationPipeline(model_path=model_path, input_size=input_size, model_size=model_size)\n",
    "\n",
    "    # if os.path.isdir(input_path):\n",
    "    #     img_list = os.listdir(input_path)\n",
    "    #     assert len(img_list) > 0\n",
    "    #     for name in tqdm(img_list):\n",
    "    #         img = cv2.imread(os.path.join(input_path, name))\n",
    "    #         image_out = colorizer.process(img)\n",
    "    #         cv2.imwrite(os.path.join(output_path, name), image_out)\n",
    "    # else:\n",
    "    #     frames = extract_frames(input_path)\n",
    "    #     processed_frames = process_frames(frames, colorizer)\n",
    "    #     create_video(processed_frames, os.path.join(video_output_path, 'output_video.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path: results\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddcolor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
